# -*- coding: utf-8 -*-
"""AFEC2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171KAfX6HDqDPT0r8zXpdjYJe7IX1lUtW
"""

import torch
import torch.nn as nn
import numpy as np
import torch.optim as opti
from torch.utils.data import DataLoader
from copy import deepcopy
from torch.nn import functional as F
from tqdm.notebook import tqdm
from torchvision import transforms
from torchvision.models import resnet18
from pyhessian import hessian
from utils import *
from nngeometry.metrics import FIM
from nngeometry.object import PMatKFAC, PMatDiag, PVector
from continuum.datasets import CIFAR10
from continuum.datasets import MNIST
from continuum import ClassIncremental
from continuum import InstanceIncremental
from continuum.datasets import PyTorchDataset
import torchvision
from nngeometry.object import PMatDense
import torchvision.models as models
import torch.nn.init as init

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU is not available")

import random
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(1)

def NewTask(taskset):
     T_Model = Net()
     T_Model = T_Model.to(device)
     T_criterion = nn.CrossEntropyLoss()
     T_optimizer = optim.SGD(T_Model.parameters(), lr=0.001)
     T_trainLoader = DataLoader(deepcopy(taskset), batch_size = 64, shuffle=True, num_workers=2)
     for epoch in range(10):
            for inputs, labels, t in T_trainLoader:
                 #Zero the parameter gradients
                 T_optimizer.zero_grad()

                 #Forward pass
                 outputs = T_Model(inputs.to(device))

                 #Calculate the loss
                 T_Loss = T_criterion(outputs, labels.to(device))

                 #Train
                 T_Loss = T_Loss.to(device)
                 T_Loss.backward()
                 T_optimizer.step()

     T_Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=True, num_workers=2)
     M_New = FIM(model = T_Model,
                          loader = T_Loader,
                          representation = PMatKFAC,
                          n_output = 10,
                          variant ='classif_logits',
                          device = 'cuda')
     v_New = PVector.from_model(model).clone().detach()
     return  v_New, M_New

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Train set
Trainset = CIFAR10(data_path="./data", download=True, train=True)
scenario1 = ClassIncremental(Trainset, increment=2)

#Test set
Testset = CIFAR10(data_path="./data", download=True, train=False)
scenario2 = ClassIncremental(Testset, increment=2)

def AFEC(model, BS, optimizer, criterion, scenario1, scenario2, Initial_WD, Lambda, beta, epoch):
  WD = Initial_WD
  FisherList = []
  A = 0
  G = []
  G_List = []

  Acc_L = []
  Acc_Train = []
  Bests = []

  F = []

  Temp_Epoch = 0

  for task_id, taskset in enumerate(scenario1):
      model.train()
      G_List = []
      if(task_id == 4):
          epoch = 50
      trainLoader = DataLoader(deepcopy(taskset), batch_size=BS, shuffle=True, num_workers=2)
      if task_id > 0:
           v_New, M_New = NewTask(taskset)
      for E in range(epoch):
            G = []
            for inputs, labels, t in trainLoader:
                 #Zero the parameter gradients
                 optimizer.zero_grad()

                 #Forward pass
                 outputs = model(inputs.to(device))

                 #Calculate the loss
                 Loss = criterion(outputs, labels.to(device))

                 v = PVector.from_model(model)
                 if(task_id == 0):
                     #Add Regularization
                     Loss = Loss + WD*(1/2)*v.dot(v)

                 else:
                     #Recover the total loss
                     for i in range(len(FisherList)):
                           Loss = Loss + Lambda * ( (1/2) * MatrixOP(FisherList[i], v, v_Last))
                     Loss = Loss +  (  (1/2) * WD * DotOP(v, v_Last) )
                     Loss = Loss + ( (beta/(1-beta)) * (1/2) * MatrixOP(M_New, v, v_New) )


                 if(task_id == 0):
                     G.append(  Grad_Monitor(model, inputs.to(device), labels.to(device), criterion) )
                 else:
                     G.append( (Grad_Monitor(model, inputs.to(device), labels.to(device), criterion) + Grad_old(FisherList, model, v_Last, Lambda)) )


                 #Train
                 Loss = Loss.to(device)
                 Loss.backward()
                 optimizer.step()

            G_List.append(np.mean(G))
            print(f"Task: {task_id}, Epoch: {E}, Gradient: {G_List[len(G_List)-1]}")

            if(task_id == 4):
                  Temp =  Acc(scenario2, model, task_id)
                  if(Temp >= A):
                        A = Temp
                        Temp_Epoch = E+1
                        Temp_v = PVector.from_model(model).clone().detach()
                        print(f"A: {A}")




      if(task_id == 4):
            Temp_v.copy_to_model(model)


      #Test the Accuracy
      Acc_L.append(Acc(scenario2, model, task_id))
      Acc_Train.append(Acc(scenario1, model, task_id))
      Bests = Bests_Update(scenario1, model, task_id, Bests)

      F.append(singleAcc(taskset, model, task_id))
      accuracies = Accuraccies(scenario1, model, task_id)
      print(f"Bests: {Bests}")
      print(f"Accuracies: {accuracies}")
      print(f"Plasticity: {F[len(F)-1]}")

      #store old information
      TempLoader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=True, num_workers=2)
      F_kfac = FIM(model = model,
                          loader = TempLoader,
                          representation = PMatKFAC,
                          n_output = 10,
                          variant ='classif_logits',
                          device = 'cuda')
      FisherList.append(F_kfac)
      v_Last = PVector.from_model(model).clone().detach()

      #print final information
      if(task_id == 4):
             AAA_Test = (1/len(Acc_L)) * np.sum(Acc_L)
             print(f"AAA_Test: {AAA_Test}")
             AAA_Train = (1/len(Acc_Train)) * np.sum(Acc_Train)
             print(f"AAA_Train: {AAA_Train}")
             FM(scenario1, model, Bests)

             PM = (1/len(F)) * np.sum(F)
             print(f"PM :{PM}")

print("AFEC")
model = Net()
model = model.to(device)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.000025)
AFEC(model, BS=16, optimizer, criterion, scenario1, scenario2, Initial_WD=0, Lambda=120, beta=0.5, epoch=50)