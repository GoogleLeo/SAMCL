# -*- coding: utf-8 -*-
"""refresh2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16vITFBPSBK7OM5hm_etAF-1aaHjHumSM
"""

import torch
import torch.nn as nn
import numpy as np
import torch.optim as opti
from torch.utils.data import DataLoader
from copy import deepcopy
from torch.nn import functional as F
from tqdm.notebook import tqdm
from torchvision import transforms
from torchvision.models import resnet18
from pyhessian import hessian
from utils import *
from nngeometry.metrics import FIM
from nngeometry.object import PMatKFAC, PMatDiag, PVector
from continuum.datasets import CIFAR10
from continuum.datasets import MNIST
from continuum import ClassIncremental
from continuum import InstanceIncremental
from continuum.datasets import PyTorchDataset
import torchvision
from nngeometry.object import PMatDense
import torchvision.models as models
import torch.nn.init as init

if torch.cuda.is_available():
    device = torch.device("cuda")
    print("GPU is available")
else:
    device = torch.device("cpu")
    print("GPU is not available")

import random
def set_seed(seed):
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

set_seed(1)

def Init_Learn(model, inputs, labels, lr):
    model.zero_grad()
    v = getParameter(model)

    #obtain the first gradient
    outputs = model(inputs)
    Loss = criterion(outputs, labels)
    Loss.backward()
    Grad = getGrad(model)

    #update
    new_v = v - (lr * Grad)
    model = LoadParameter(model, new_v)
    model.zero_grad()

    return model

def unLearn(model, inputs, labels, criterion, gamma, FisherLast, v_Last, Lambda):
    model.zero_grad()
    v = PVector.from_model(model).clone().get_flat_representation().detach()
    Inverse_Fisher = 1.0/FisherLast
    Inverse_Fisher = (1/Lambda) * Inverse_Fisher



    #obtain the first gradient
    outputs = model(inputs)
    Loss = criterion(outputs, labels)
    Loss.backward()
    Grad1 = PVector.from_model_grad(model).clone().get_flat_representation().detach()
    Grad1 = Inverse_Fisher * Grad1

    #obtain the second gradient
    Grad2 =  v - v_Last

    #obtain the final gradient
    Grad = Grad1 + Grad2

    #obtain the noise
    Noise = torch.normal(torch.zeros(len(v)).to(device), torch.sqrt(2 * gamma * Inverse_Fisher).to(device) )

    #update
    new_v = v + (gamma * Grad) + Noise

    model = LoadParameter(model, new_v)
    model.zero_grad()

    return model


def reLearn(model, v1, inputs, labels, criterion, lr, FisherLast, v_Last, Lambda):
    model.zero_grad()
    v2 = PVector.from_model(model).clone().get_flat_representation().detach()

    #obtain the first gradient
    outputs = model(inputs)
    Loss = criterion(outputs, labels)
    Loss.backward()
    Grad1 = PVector.from_model_grad(model).clone().get_flat_representation().detach()


    #obtain the gradient of the old tasks
    Grad2 = Lambda * (FisherLast * (v2 - v_Last))

    #Final Gradient
    Grad = Grad1 + Grad2

    #Update the model
    new_v = v1 - (lr * Grad)
    model = LoadParameter(model, new_v)
    model.zero_grad()

    return model


def storeInfo(model, taskset, FisherLast):

    #store the old parameter
    v_Last = PVector.from_model(model).clone().get_flat_representation().detach()

    #store the fisher information
    TempLoader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=False, num_workers=2)
    M = FIM(model=model,
            loader=TempLoader,
            representation = PMatDiag,
            n_output=10,
            variant='classif_logits',
            device='cuda').get_diag().detach()


    #update the fisher
    FisherLast = FisherLast + M

    return FisherLast, v_Last

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
        self.fc1 = nn.Linear(64 * 8 * 8, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = self.pool(nn.functional.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8)
        x = nn.functional.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Train set
Trainset = CIFAR10(data_path="./data", download=True, train=True)
scenario1 = ClassIncremental(Trainset, increment=2)

#Test set
Testset = CIFAR10(data_path="./data", download=True, train=False)
scenario2 = ClassIncremental(Testset, increment=2)

def Refresh_CL(model, BS, gamma, lr, Lambda, criterion, scenario1, scenario2, epoch_train, epoch_unlearn):
   v = getParameter(model)
   G = []
   FisherLast = torch.ones_like(v)
   v_Last = None
   A = 0

   Acc_L = []
   Acc_Train = []
   Bests = []


   for task_id, taskset in enumerate(scenario1):
        model.train()
        print(f"Current Task ID: {task_id}")
        trainLoader = DataLoader(deepcopy(taskset), batch_size = BS, shuffle=True, num_workers=2)
        if(task_id == 4):
            epoch = 50
        for E in range(epoch_train):
            G = []
            for inputs, labels, t in trainLoader:
                if(task_id == 0):
                    model = Init_Learn(model, inputs.to(device), labels.to(device), lr)
                else:
                    v1 = getParameter(model)
                    for i in range(epoch_unlearn):
                        model = unLearn(model, inputs.to(device), labels.to(device), criterion, gamma, FisherLast, v_Last, Lambda)
                    model = reLearn(model, v1, inputs.to(device), labels.to(device), criterion, lr, FisherLast, v_Last, Lambda)
                G.append(Grad_Monitor(model, inputs.to(device), labels.to(device), criterion))

            #Print the Gradient Norm and the distance
            G_New = np.mean(G)
            print(f"Task: {task_id}, Epoch: {E}, Grad_New: {G_New}" )


            if(task_id == 4):
                  Temp =  Acc(scenario2, model, task_id)
                  if(Temp >= A):
                        A = Temp
                        Temp_v = getParameter(model)
                        print(f"A : {A}")

        if(task_id == 4):
              model = LoadParameter(model, Temp_v)


        #Test the Accuracy
        Acc_L.append(Acc(scenario2, model, task_id))
        Acc_Train.append(Acc(scenario1, model, task_id))
        Bests = Bests_Update(scenario1, model, task_id, Bests)


        print(f"Bests: {Bests}")
        #store old information
        FisherLast, v_Last = storeInfo(model, taskset, FisherLast, tau)

        if(task_id == 4):
             AAA_Test = (1/len(Acc_L)) * np.sum(Acc_L)
             print(f"AAA_Test: {AAA_Test}")
             AAA_Train = (1/len(Acc_Train)) * np.sum(Acc_Train)
             print(f"AAA_Train: {AAA_Train}")
             FM(scenario1, model, Bests)

print("Refresh_CL")
model = Net()
model = model.to(device)
criterion = nn.CrossEntropyLoss()
Refresh_CL(model, gamma=0.000025, lr=0.000025, Lambda=120, criterion, scenario1, scenario2, epoch_train=50, epoch_unlearn=1)