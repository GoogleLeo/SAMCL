# -*- coding: utf-8 -*-
"""utils.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MwIlRI3P8fhbUyrYcOFDV_k3aMCxAaF5
"""

import torch
import torch.nn as nn
import numpy as np
import torch.optim as optim
from torch.utils.data import DataLoader
from copy import deepcopy
from torch.nn import functional as F
from tqdm.notebook import tqdm
from torchvision import transforms
import torchvision
import torchvision.models as models
import torch.nn.init as init

def MatrixOP(F, v, v_Last):
     a = F.mv(v)
     return F.vTMv(v) + F.vTMv(v_Last) - 2*a.dot(v_Last)

def DotOP(v, v_Last):
     return v.dot(v) + v_Last.dot(v_Last) - 2*v.dot(v_Last)

def getGrad(model):
    grads = []
    for param in model.parameters():
        if param.grad is not None:
            grads.append(param.grad.view(-1))  # Flatten the gradient tensor

    gradient_vector = torch.cat(grads)
    return gradient_vector

def getParameter(model):
    params = []
    for param in model.parameters():
          params.append(param.view(-1))  # Flatten the parameter tensor

    parameter_vector = torch.cat(params)
    return parameter_vector

def LoadParameter(model, v):
    index = 0
    for param in model.parameters():
        param_length = param.numel()  # Number of elements in the parameter tensor
        param.data = v[index:index + param_length].view(param.size())
        index += param_length
    return model


def Grad_Monitor(model, inputs, labels, criterion):
    #set status
    model.eval()
    model.zero_grad()

    #Forward Pass
    outputs = model(inputs)
    Loss = criterion(outputs, labels)
    Loss.backward()

    #Calculation Grad of new task
    Grad1 = getGrad(model)
    Norm = torch.norm(Grad1, p = 2).item()

    #recover status
    model.zero_grad()
    model.train()

    return Norm

def Acc(scenario2, model, N):
     #set status
     model.eval()
     Test_TA = 0.0
     for task_id, taskset in enumerate(scenario2):
         Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=False, num_workers=2)
         # Make predictions
         for inputs, labels, y in Loader:
              outputs = model(inputs.to(device))
              _, predicted = torch.max(outputs, dim=1)
              accuracy = (1/len(inputs))*((predicted == labels.to(device)).sum().item())

         Test_TA = Test_TA + accuracy

         if(task_id == N):
              break

     Acc = (1/(N+1)) * Test_TA
     print(f"Acc: {Acc}")
     model.train()

     return Acc


def Bests_Update(scenario2, model, N, Bests):
    model.eval()
    for task_id, taskset in enumerate(scenario2):
         Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=False, num_workers=2)
         # Make predictions
         for inputs, labels, y in Loader:
              outputs = model(inputs.to(device))
              _, predicted = torch.max(outputs, dim=1)
              accuracy = (1/len(inputs))*((predicted == labels.to(device)).sum().item())

         if(task_id == N):
              Bests.append(accuracy)
              break
         elif(Bests[task_id] < accuracy):
                Bests[task_id] = accuracy
    model.train()
    return Bests


def FM(scenario2, model, Bests):
     model.eval()
     FM = 0.0
     for task_id, taskset in enumerate(scenario2):
         Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=False, num_workers=2)
         # Make predictions
         for inputs, labels, y in Loader:
              outputs = model(inputs.to(device))
              _, predicted = torch.max(outputs, dim=1)
              accuracy = (1/len(inputs))*((predicted == labels.to(device)).sum().item())

         FM = FM + (accuracy - Bests[task_id])

     FM = 1/(len(scenario2)) * FM

     print(f"FM: {FM}")
     model.train()


def Accuraccies(scenario, model, N):
      model.eval()
      A = []
      for task_id, taskset in enumerate(scenario):
          Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=False, num_workers=2)
          # Make predictions
          for inputs, labels, y in Loader:
              outputs = model(inputs.to(device))
              _, predicted = torch.max(outputs, dim=1)
              accuracy = (1/len(inputs))*((predicted == labels.to(device)).sum().item())
          A.append(accuracy)
          if(task_id == N):
               break

      model.train()

      return A


def singleAcc(taskset, model, N):
    acc = [0.7976,   0.6727,    0.6833,  0.7813,  0.7663]
    Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=False, num_workers=2)
    for inputs, labels, y in Loader:
          outputs = model(inputs.to(device))
          _, predicted = torch.max(outputs, dim=1)
          accuracy = (1/len(inputs))*((predicted == labels.to(device)).sum().item())

    return accuracy - acc[N]


def Final_Test(scenario2, model):
    model.eval()
    #Test over the testset
    Test_accuracies = []
    Test_TA = 0.0
    for task_id, taskset in enumerate(scenario2):
         Loader = DataLoader(deepcopy(taskset), batch_size=len(taskset), shuffle=True, num_workers=2)
         # Make predictions
         for inputs, labels, y in Loader:
              outputs = model(inputs.to(device))
              _, predicted = torch.max(outputs, dim=1)
              accuracy = (1/len(inputs))*((predicted == labels.to(device)).sum().item())

         Test_accuracies.append(accuracy)
         Test_TA = Test_TA + accuracy

    Test_TA = (1/len(scenario2)) * Test_TA

    print(f"Test Accuracy: {Test_TA}")

    return Test_TA